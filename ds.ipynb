{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Datasets\n",
    "---\n",
    "\n",
    "This notebook is supposed to give you an idea how you can define custom datasets when working with pytorch, and how you can perform train-test-splitting on your data. We start out with a very simple ficticious dataset of 100 samples and 10 features, stored in a 100 x 10 array. We generate the data points randomly and write the resulting numpy array to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Sequence, Optional\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./data\"):\n",
    "    os.mkdir(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datapath = \"data/all.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.rand(100, 10)\n",
    "np.save(all_datapath, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ = np.load(all_datapath)\n",
    "assert np.all(data == data_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem: \n",
    "How do we create a Dataset for reading the data from this? And how do we split it into train, valid and test?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1: Splitting the data on disk\n",
    "\n",
    "To prepare the data in a convenient way for further usage by a Pytorch model, we define a `Dataset` class. A `Dataset` is essentially just a container that allows you to interface with your data, however it is provided. All it must provide is an `__init__()` method for instantiation, where the arguments can be complety arbitrary, a `___len__()` method that returns the length of a dataset, and a `__getitem__()` method that receives an integer index (or a sequence thereof if you want to allow for advanced indexing) and returns the respective item from the dataset as tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset_path: str):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.data = np.load(dataset_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return torch.from_numpy(self.data[idx, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_idx = np.arange(0, len(data))\n",
    "train_split, valid_test_split = train_test_split(all_idx, train_size=0.8)\n",
    "valid_split, test_split = train_test_split(valid_test_split, train_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[train_split, :]\n",
    "train_datapath = \"data/train.npy\"\n",
    "np.save(train_datapath, train_data)\n",
    "\n",
    "valid_data = data[valid_split, :]\n",
    "valid_datapath = \"data/valid.npy\"\n",
    "np.save(valid_datapath, valid_data)\n",
    "\n",
    "test_data = data[test_split, :]\n",
    "test_datapath = \"data/test.npy\"\n",
    "np.save(test_datapath, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = CustomDataset(train_datapath)\n",
    "valid_ds = CustomDataset(valid_datapath)\n",
    "test_ds = CustomDataset(test_datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 10, 10)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds), len(valid_ds), len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2658, 0.1320, 0.9718, 0.9078, 0.3574, 0.5475, 0.2282, 0.1728, 0.0578,\n",
       "         0.3115],\n",
       "        [0.4351, 0.3993, 0.6958, 0.6714, 0.8000, 0.7195, 0.4709, 0.8084, 0.6217,\n",
       "         0.7403],\n",
       "        [0.2238, 0.1720, 0.8436, 0.8522, 0.8580, 0.5453, 0.4868, 0.4745, 0.2917,\n",
       "         0.2008],\n",
       "        [0.1910, 0.4369, 0.3929, 0.1948, 0.2055, 0.3353, 0.3057, 0.3246, 0.9288,\n",
       "         0.5554],\n",
       "        [0.5993, 0.8189, 0.0609, 0.0548, 0.4448, 0.8515, 0.1662, 0.0251, 0.3644,\n",
       "         0.7465],\n",
       "        [0.8132, 0.5710, 0.4544, 0.2389, 0.9538, 0.1128, 0.9511, 0.2938, 0.9022,\n",
       "         0.1942],\n",
       "        [0.8564, 0.2553, 0.4450, 0.7629, 0.8897, 0.2289, 0.7435, 0.1221, 0.2614,\n",
       "         0.7545],\n",
       "        [0.8919, 0.4885, 0.6227, 0.5590, 0.7655, 0.9950, 0.4389, 0.5561, 0.7542,\n",
       "         0.6109],\n",
       "        [0.2427, 0.8769, 0.0546, 0.0748, 0.6120, 0.3136, 0.4596, 0.2031, 0.1239,\n",
       "         0.4740],\n",
       "        [0.0090, 0.6527, 0.5670, 0.3864, 0.0346, 0.4207, 0.3608, 0.9819, 0.0493,\n",
       "         0.0180]], dtype=torch.float64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_ds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 10 is out of bounds for axis 0 with size 10",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m valid_ds[\u001b[39m10\u001b[39;49m]\n",
      "Cell \u001b[0;32mIn[18], line 10\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx: \u001b[39mint\u001b[39m):\n\u001b[0;32m---> 10\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mfrom_numpy(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata[idx, :])\n",
      "\u001b[0;31mIndexError\u001b[0m: index 10 is out of bounds for axis 0 with size 10"
     ]
    }
   ],
   "source": [
    "valid_ds[10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have successfully created a dataset for training, validation and testing, let us wrap the datasets with a DataLoader so we can iterate over the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=4, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=4, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=4, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any number of `DataLoader`s can be created on a dataset. Think of a `DataLoader` as a consumable that allows you to iterate over all data points of a data set exactly once and is then used up. So, to iterate over a dataset multiple times, you would define multiple dataloaders. \n",
    "\n",
    "When creating a `DataLoader`, setting the `shuffle` parameter to `False` will respect the order of the dataset, while setting it to `True` will iterate over the dataset in a random order. See for yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6604, 0.3281, 0.3410, 0.1327, 0.6157, 0.9070, 0.3888, 0.4450, 0.9115,\n",
      "         0.7691],\n",
      "        [0.1119, 0.4702, 0.8129, 0.9292, 0.7062, 0.4830, 0.5732, 0.3643, 0.4619,\n",
      "         0.6012],\n",
      "        [0.1041, 0.4542, 0.7056, 0.7685, 0.2268, 0.3509, 0.4818, 0.4445, 0.2015,\n",
      "         0.5644],\n",
      "        [0.3941, 0.7311, 0.0825, 0.2191, 0.7799, 0.7084, 0.4344, 0.9510, 0.4570,\n",
      "         0.6801]], dtype=torch.float64)\n",
      "tensor([[0.7908, 0.6135, 0.0877, 0.2364, 0.0934, 0.8608, 0.4089, 0.1683, 0.6018,\n",
      "         0.3265],\n",
      "        [0.4291, 0.1776, 0.1431, 0.0675, 0.4716, 0.3212, 0.6341, 0.9953, 0.3252,\n",
      "         0.9552],\n",
      "        [0.6680, 0.6070, 0.1877, 0.4678, 0.7224, 0.9612, 0.2207, 0.6191, 0.3543,\n",
      "         0.1904],\n",
      "        [0.4047, 0.6705, 0.1524, 0.3851, 0.7845, 0.9092, 0.6224, 0.5635, 0.7001,\n",
      "         0.5579]], dtype=torch.float64)\n",
      "tensor([[0.5603, 0.4253, 0.0136, 0.5578, 0.4908, 0.8270, 0.5704, 0.6199, 0.7504,\n",
      "         0.2012],\n",
      "        [0.0545, 0.3724, 0.2470, 0.3543, 0.3972, 0.2273, 0.4405, 0.2374, 0.9427,\n",
      "         0.8252]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for batch in iter(test_dl):\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4047, 0.6705, 0.1524, 0.3851, 0.7845, 0.9092, 0.6224, 0.5635, 0.7001,\n",
      "         0.5579],\n",
      "        [0.6604, 0.3281, 0.3410, 0.1327, 0.6157, 0.9070, 0.3888, 0.4450, 0.9115,\n",
      "         0.7691],\n",
      "        [0.3941, 0.7311, 0.0825, 0.2191, 0.7799, 0.7084, 0.4344, 0.9510, 0.4570,\n",
      "         0.6801],\n",
      "        [0.0545, 0.3724, 0.2470, 0.3543, 0.3972, 0.2273, 0.4405, 0.2374, 0.9427,\n",
      "         0.8252]], dtype=torch.float64)\n",
      "tensor([[0.1041, 0.4542, 0.7056, 0.7685, 0.2268, 0.3509, 0.4818, 0.4445, 0.2015,\n",
      "         0.5644],\n",
      "        [0.7908, 0.6135, 0.0877, 0.2364, 0.0934, 0.8608, 0.4089, 0.1683, 0.6018,\n",
      "         0.3265],\n",
      "        [0.1119, 0.4702, 0.8129, 0.9292, 0.7062, 0.4830, 0.5732, 0.3643, 0.4619,\n",
      "         0.6012],\n",
      "        [0.5603, 0.4253, 0.0136, 0.5578, 0.4908, 0.8270, 0.5704, 0.6199, 0.7504,\n",
      "         0.2012]], dtype=torch.float64)\n",
      "tensor([[0.4291, 0.1776, 0.1431, 0.0675, 0.4716, 0.3212, 0.6341, 0.9953, 0.3252,\n",
      "         0.9552],\n",
      "        [0.6680, 0.6070, 0.1877, 0.4678, 0.7224, 0.9612, 0.2207, 0.6191, 0.3543,\n",
      "         0.1904]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "test_dl = DataLoader(test_ds, batch_size=4, shuffle=True)\n",
    "for batch in iter(test_dl):\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1119, 0.4702, 0.8129, 0.9292, 0.7062, 0.4830, 0.5732, 0.3643, 0.4619,\n",
      "         0.6012],\n",
      "        [0.1041, 0.4542, 0.7056, 0.7685, 0.2268, 0.3509, 0.4818, 0.4445, 0.2015,\n",
      "         0.5644],\n",
      "        [0.7908, 0.6135, 0.0877, 0.2364, 0.0934, 0.8608, 0.4089, 0.1683, 0.6018,\n",
      "         0.3265],\n",
      "        [0.3941, 0.7311, 0.0825, 0.2191, 0.7799, 0.7084, 0.4344, 0.9510, 0.4570,\n",
      "         0.6801]], dtype=torch.float64)\n",
      "tensor([[0.4291, 0.1776, 0.1431, 0.0675, 0.4716, 0.3212, 0.6341, 0.9953, 0.3252,\n",
      "         0.9552],\n",
      "        [0.4047, 0.6705, 0.1524, 0.3851, 0.7845, 0.9092, 0.6224, 0.5635, 0.7001,\n",
      "         0.5579],\n",
      "        [0.6604, 0.3281, 0.3410, 0.1327, 0.6157, 0.9070, 0.3888, 0.4450, 0.9115,\n",
      "         0.7691],\n",
      "        [0.5603, 0.4253, 0.0136, 0.5578, 0.4908, 0.8270, 0.5704, 0.6199, 0.7504,\n",
      "         0.2012]], dtype=torch.float64)\n",
      "tensor([[0.6680, 0.6070, 0.1877, 0.4678, 0.7224, 0.9612, 0.2207, 0.6191, 0.3543,\n",
      "         0.1904],\n",
      "        [0.0545, 0.3724, 0.2470, 0.3543, 0.3972, 0.2273, 0.4405, 0.2374, 0.9427,\n",
      "         0.8252]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "test_dl = DataLoader(test_ds, batch_size=4, shuffle=False)\n",
    "for batch in iter(test_dl):\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.11189311 0.47022563 0.81289964 0.92920906 0.7061915  0.48302178\n",
      "  0.57320662 0.364294   0.4618689  0.60119647]\n",
      " [0.104106   0.45422257 0.70556399 0.76854487 0.22682721 0.35090406\n",
      "  0.48183919 0.44451674 0.20153437 0.56440944]\n",
      " [0.79082651 0.61350899 0.08772634 0.23642197 0.09341591 0.86080943\n",
      "  0.4089348  0.16828306 0.60179675 0.32645062]\n",
      " [0.39412752 0.73113621 0.08249928 0.21905882 0.77990598 0.70838133\n",
      "  0.43437863 0.95097075 0.45703167 0.68014718]\n",
      " [0.42905397 0.17759902 0.14309815 0.06750697 0.47163401 0.3211667\n",
      "  0.63414452 0.99529385 0.32516169 0.95521448]\n",
      " [0.40473015 0.67045864 0.1523741  0.38511324 0.78445348 0.90923892\n",
      "  0.62237947 0.56351123 0.70009889 0.55794555]\n",
      " [0.66037796 0.32813615 0.34095467 0.13273115 0.61572013 0.90702802\n",
      "  0.3887919  0.44501581 0.91153967 0.76910439]\n",
      " [0.56028702 0.42526807 0.01361456 0.55777071 0.49075718 0.82702142\n",
      "  0.57035887 0.61988486 0.75041955 0.20122308]\n",
      " [0.66802578 0.6069664  0.18773405 0.46781397 0.72241543 0.96118654\n",
      "  0.2207421  0.61914169 0.35426089 0.19040513]\n",
      " [0.05450969 0.37241254 0.24704333 0.35433493 0.39721119 0.22733226\n",
      "  0.440544   0.23743294 0.94267803 0.82524225]]\n"
     ]
    }
   ],
   "source": [
    "print(test_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2: The elegant way\n",
    "\n",
    "Mask out what you don't want to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset_path: str, subset: Optional[Sequence[int]] = None):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.data = np.load(dataset_path)\n",
    "        self.subset = np.array([*subset], dtype=np.int64) if subset is not None else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0] if self.subset is None else len(self.subset)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        if self.subset is not None:\n",
    "            idx = self.subset[idx]\n",
    "        return torch.from_numpy(self.data[idx, :])\n",
    "\n",
    "    def get_subset(self, subset: Optional[Sequence[int]]) -> 'CustomDataset':\n",
    "        \"\"\" Returns a new CustomDataset using only a subset of indices. \"\"\"\n",
    "        if self.subset is not None:\n",
    "            subset = self.subset[subset]\n",
    "        return CustomDataset(self.dataset_path, subset=subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = CustomDataset(all_datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.9730, 0.8578, 0.8550, 0.3406, 0.5674, 0.8235, 0.5873, 0.8169, 0.0621,\n",
       "         0.4731], dtype=torch.float64),\n",
       " tensor([0.4351, 0.3993, 0.6958, 0.6714, 0.8000, 0.7195, 0.4709, 0.8084, 0.6217,\n",
       "         0.7403], dtype=torch.float64))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0], ds[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_idx = np.arange(0, len(ds))\n",
    "train_split, valid_test_split = train_test_split(all_idx, train_size=0.8)\n",
    "valid_split, test_split = train_test_split(valid_test_split, train_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([49, 91, 46, 61, 70, 60, 58,  4, 64, 87, 38, 95, 97, 81, 89, 62, 48,\n",
       "       47, 66, 65, 17, 14, 42, 92, 83, 50, 41, 93, 52, 80, 99, 67, 84, 27,\n",
       "       90, 40, 94, 29,  5, 22, 21,  1, 26, 57,  0, 98, 30, 68, 37, 10, 51,\n",
       "       39, 63, 18,  8, 35, 19, 78, 86, 77, 20,  3, 25, 71, 15, 16, 55, 12,\n",
       "       23, 36, 96, 24, 32, 69, 73, 85, 11,  2, 75, 45])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6,  7,  9, 76, 44, 74, 33, 31, 13, 28])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([54, 72, 43, 79, 34, 88, 56, 82, 53, 59])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ds.get_subset(subset=train_split)\n",
    "valid_ds = ds.get_subset(subset=valid_split)\n",
    "test_ds = ds.get_subset(subset=test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 10, 10)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds), len(valid_ds), len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.4165e-01, 8.8414e-01, 1.0307e-01, 6.8394e-01, 7.5107e-02, 4.4322e-01,\n",
       "         9.7007e-01, 6.3511e-01, 9.6964e-01, 4.3268e-04],\n",
       "        [8.8705e-02, 4.9055e-01, 6.2738e-01, 3.4679e-02, 1.6851e-01, 5.0979e-01,\n",
       "         4.8836e-02, 1.7073e-02, 5.5715e-01, 2.9179e-01],\n",
       "        [7.5236e-01, 1.3519e-01, 6.4312e-01, 3.8824e-01, 2.8274e-01, 4.0763e-01,\n",
       "         9.3979e-01, 9.7585e-01, 4.5837e-03, 6.0436e-01],\n",
       "        [5.7535e-02, 1.0731e-01, 4.3320e-01, 4.1618e-01, 7.2452e-01, 1.4872e-01,\n",
       "         4.5900e-01, 6.8188e-01, 5.3524e-01, 2.2020e-02],\n",
       "        [2.7957e-01, 9.1127e-01, 2.2009e-01, 7.0582e-01, 3.1125e-01, 1.9890e-01,\n",
       "         5.8782e-02, 6.9470e-01, 3.1745e-01, 9.6368e-01],\n",
       "        [1.0411e-01, 4.5422e-01, 7.0556e-01, 7.6854e-01, 2.2683e-01, 3.5090e-01,\n",
       "         4.8184e-01, 4.4452e-01, 2.0153e-01, 5.6441e-01],\n",
       "        [2.3036e-01, 3.0045e-01, 8.3751e-02, 3.1431e-02, 3.9653e-01, 2.0653e-01,\n",
       "         9.3354e-01, 5.7006e-01, 3.8842e-01, 7.5189e-01],\n",
       "        [2.3922e-01, 3.0009e-02, 9.6230e-01, 6.6515e-01, 2.1838e-01, 1.7579e-01,\n",
       "         4.7618e-02, 1.1347e-01, 1.2134e-02, 1.8978e-01],\n",
       "        [8.2997e-01, 1.9476e-01, 2.9357e-01, 6.6820e-01, 7.5377e-01, 7.3101e-01,\n",
       "         4.8301e-01, 8.1237e-01, 5.2901e-01, 7.0016e-01],\n",
       "        [8.6663e-01, 8.7920e-01, 3.1132e-01, 5.4982e-01, 5.3397e-01, 1.5751e-02,\n",
       "         1.1824e-01, 9.7615e-01, 4.4857e-01, 3.9332e-01]], dtype=torch.float64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 10 is out of bounds for axis 0 with size 10",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_ds[\u001b[39m10\u001b[39;49m]\n",
      "Cell \u001b[0;32mIn[30], line 13\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx: \u001b[39mint\u001b[39m):\n\u001b[1;32m     12\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubset \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 13\u001b[0m         idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubset[idx]\n\u001b[1;32m     14\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mfrom_numpy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[idx, :])\n",
      "\u001b[0;31mIndexError\u001b[0m: index 10 is out of bounds for axis 0 with size 10"
     ]
    }
   ],
   "source": [
    "test_ds[10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we could use this technique to further subset the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_test_ds = test_ds.get_subset([0, 1, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.4165e-01, 8.8414e-01, 1.0307e-01, 6.8394e-01, 7.5107e-02, 4.4322e-01,\n",
       "         9.7007e-01, 6.3511e-01, 9.6964e-01, 4.3268e-04],\n",
       "        [8.8705e-02, 4.9055e-01, 6.2738e-01, 3.4679e-02, 1.6851e-01, 5.0979e-01,\n",
       "         4.8836e-02, 1.7073e-02, 5.5715e-01, 2.9179e-01],\n",
       "        [8.2997e-01, 1.9476e-01, 2.9357e-01, 6.6820e-01, 7.5377e-01, 7.3101e-01,\n",
       "         4.8301e-01, 8.1237e-01, 5.2901e-01, 7.0016e-01]], dtype=torch.float64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_test_ds[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for axis 0 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_test_ds[\u001b[39m3\u001b[39;49m]\n",
      "Cell \u001b[0;32mIn[30], line 13\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx: \u001b[39mint\u001b[39m):\n\u001b[1;32m     12\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubset \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 13\u001b[0m         idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubset[idx]\n\u001b[1;32m     14\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mfrom_numpy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[idx, :])\n",
      "\u001b[0;31mIndexError\u001b[0m: index 3 is out of bounds for axis 0 with size 3"
     ]
    }
   ],
   "source": [
    "test_test_ds[3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with our previous datasets, we can define DataLoaders to iterate through all samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=4, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=4, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1041, 0.4542, 0.7056, 0.7685, 0.2268, 0.3509, 0.4818, 0.4445, 0.2015,\n",
      "         0.5644],\n",
      "        [0.2304, 0.3005, 0.0838, 0.0314, 0.3965, 0.2065, 0.9335, 0.5701, 0.3884,\n",
      "         0.7519],\n",
      "        [0.0887, 0.4905, 0.6274, 0.0347, 0.1685, 0.5098, 0.0488, 0.0171, 0.5572,\n",
      "         0.2918],\n",
      "        [0.2392, 0.0300, 0.9623, 0.6651, 0.2184, 0.1758, 0.0476, 0.1135, 0.0121,\n",
      "         0.1898]], dtype=torch.float64)\n",
      "tensor([[5.4165e-01, 8.8414e-01, 1.0307e-01, 6.8394e-01, 7.5107e-02, 4.4322e-01,\n",
      "         9.7007e-01, 6.3511e-01, 9.6964e-01, 4.3268e-04],\n",
      "        [5.7535e-02, 1.0731e-01, 4.3320e-01, 4.1618e-01, 7.2452e-01, 1.4872e-01,\n",
      "         4.5900e-01, 6.8188e-01, 5.3524e-01, 2.2020e-02],\n",
      "        [2.7957e-01, 9.1127e-01, 2.2009e-01, 7.0582e-01, 3.1125e-01, 1.9890e-01,\n",
      "         5.8782e-02, 6.9470e-01, 3.1745e-01, 9.6368e-01],\n",
      "        [7.5236e-01, 1.3519e-01, 6.4312e-01, 3.8824e-01, 2.8274e-01, 4.0763e-01,\n",
      "         9.3979e-01, 9.7585e-01, 4.5837e-03, 6.0436e-01]], dtype=torch.float64)\n",
      "tensor([[0.8300, 0.1948, 0.2936, 0.6682, 0.7538, 0.7310, 0.4830, 0.8124, 0.5290,\n",
      "         0.7002],\n",
      "        [0.8666, 0.8792, 0.3113, 0.5498, 0.5340, 0.0158, 0.1182, 0.9761, 0.4486,\n",
      "         0.3933]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for batch in iter(test_dl):\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-toy-V1aLj7GV-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
